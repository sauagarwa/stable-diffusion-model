{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca437184-781e-4806-8e13-f57f88f1cb49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b836c43e-c984-4661-8acd-cc3917243eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /opt/app-root/src/stable-diffusion-model\n"
     ]
    }
   ],
   "source": [
    "work_dir=os.getcwd()\n",
    "print(f\"Current working directory: {work_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20fb37d-b1d9-4c92-9b87-d9fa64c647e0",
   "metadata": {},
   "source": [
    "### Apply role and custom roles manually\n",
    "\n",
    "1. Apply the deployment/role.yaml file to define roles. Update the namespace in the role.yaml file\n",
    "2. Apply the deployment/role-binding.yaml file to bind role to the service account. Replace the name of the workbench and namespace in the role-binding.yaml file\n",
    "3. Create a data connection with the name stable-diffusion to point to the s3 bucket\n",
    "4. Update the tolerations in the deployment/inference-service.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12967ec8-1045-4631-b4b7-40be2dd71ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "toleration=os.environ.get('toleration', 'odh-notebook')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81158802-04cf-483b-83dd-2f446512a092",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "servingruntime.serving.kserve.io \"stable-diffusion\" deleted\n",
      "Error from server (NotFound): inferenceservices.serving.kserve.io \"stable-diffusion\" not found\n",
      "invoking template:deployment/serving-runtime.yaml\n",
      "system:serviceaccount:worldline:pytorch\n",
      "<_io.BufferedReader name=72>\n",
      "Deployed template deployment/serving-runtime.yaml.\n",
      "invoking template:deployment/inference-service.yaml\n",
      "system:serviceaccount:worldline:pytorch\n",
      "<_io.BufferedReader name=72>\n",
      "Deployed template deployment/inference-service.yaml.\n"
     ]
    }
   ],
   "source": [
    "# This can take up to 10mins to be ready\n",
    "# oc get pods or check the kserve-container logs\n",
    "!oc delete servingruntime stable-diffusion\n",
    "!oc delete inferenceservice stable-diffusion\n",
    "\n",
    "template_data = {\"toleration\": toleration}\n",
    "\n",
    "def deploy_template(filename, template_data):\n",
    "    print(\"invoking template:\" + filename)\n",
    "    template = Template(open(filename).read())\n",
    "    rendered_template = template.render(template_data)\n",
    "\n",
    "    subprocess.run(['oc', 'whoami'])\n",
    "    ps = subprocess.Popen(['echo', rendered_template], stdout=subprocess.PIPE)\n",
    "    print(ps.stdout)\n",
    "    output = subprocess.check_output(['oc', 'apply', '-f', '-'], stdin=ps.stdout)\n",
    "    ps.wait()\n",
    "    print(f\"Deployed template {filename}.\")\n",
    "\n",
    "templates = ['deployment/serving-runtime.yaml','deployment/inference-service.yaml']\n",
    "for t in templates:\n",
    "    deploy_template(t, template_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd40888-b523-4545-ae29-7c13ed95dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!until oc get inferenceservice stable-diffusion -o jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}' | grep -q True ; do \\\n",
    "   echo \"Waiting for inference service to be ready...\"; \\\n",
    "   sleep 30; \\\n",
    "done\n",
    "\n",
    "print(\"Inference serving is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3523c7d-f60f-4cee-803e-7059e1f5b944",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = !oc get inferenceservice stable-diffusion -o jsonpath='{.status.components.predictor.url}'\n",
    "model_url = model_url.n\n",
    "\n",
    "print(f\"Model endpoint: {model_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c999509-c6ec-4d42-be77-a1068eca6086",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_token = 'unqtkn'\n",
    "class_name = 'dog'\n",
    "my_prompt = f'photo of {uniq_token} {class_name} in a sand castle'\n",
    "\n",
    "!python {work_dir}/kserve-query.py --url=\"{model_url}/v1/models/stable-diffusion:predict\" --prompt \"{my_prompt}\" --filename output.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742778d-42a0-42cf-a8ff-1bfaecfa692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename='output.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
